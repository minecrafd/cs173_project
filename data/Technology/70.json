{"title": "从编解码和词嵌入开始，一步一步理解Transformer，注意力机制(Attention)的本质是卷积神经网络(CNN)", "stat": {"aid": 1052892976, "view": 26911, "danmaku": 207, "reply": 205, "favorite": 4107, "coin": 2908, "share": 401, "now_rank": 0, "his_rank": 0, "like": 2820, "dislike": 0, "vt": 0, "vv": 26911}, "comments": [{"content": "44:22 那里Din的T行，是指T个单词吗？", "like": 0}, {"content": "🐂🐂🐂🐂🐂🐂🐂🐂🐂🐂🐂", "like": 0}, {"content": "你来了，你终于来了。", "like": 0}, {"content": "期待一手Mamba[星星眼]", "like": 0}, {"content": "https://www.bilibili.com/video/BV1XH4y1T76e?t=1171.8\n我学工科代数的时候就有这种感觉", "like": 0}, {"content": "讲的真好，十分感谢！", "like": 0}, {"content": "位置编码那里听不懂了", "like": 0}, {"content": "前面的矩阵变换可以看三蓝一棕的线代视频， 36:55 是扩散模型吗[思考]", "like": 0}, {"content": "每次看王老师的视频就像看了一篇顶级的综述", "like": 0}, {"content": "RNN和核方法也非常有兴趣。", "like": 0}, {"content": "非常感谢UP，讲解的太好了，让我理解了很多抽象的概念，这样的良心UP如今不多了，三连支持，再次感谢，希望你继续做下去，帮助更多的人！[鸡腿][鸡腿][鸡腿]", "like": 1}, {"content": "确实是更复杂的CNN，但倒不如说是CNN和RNN的融合体。吸取CNN的高效并行计算，结合RNN对每一阶段隐藏层输出的全局考虑，进而让transformer拥有更多的拟合方法选择，我觉得这才是相较于它之前的本质突破。", "like": 0}, {"content": "这时长有点恐怖", "like": 0}, {"content": "老师，矩阵对称等于说转换后向量内积不变？你这没法证明吧，比如二维对称矩阵{a，b，b，a}与向量W点积后获得{a1，b1，b1，a1}，如何证明a*b=a1*b1？这明显不对啊，除非矩阵w为单位矩阵，否则上式不可能成立。", "like": 0}, {"content": "关于注意力的解释讲的太好了，不过当初发明注意力的人，是先想到了解释，在构造出模型，还是先构造出了模型，再给予对应的解释？", "like": 0}, {"content": "当时我对transformer大致上的理解和up是大致类似的，但是没有想到up可以给我补充很多细节，比如位置编码当初理解上就没有考虑过傅里叶级数，而是单纯想到一个核函数变化得到高维空间的位置表达。可能还是论文看少了吧[笑哭]", "like": 0}, {"content": "感觉transformer比较之前的RNN最大的优势是可以并行计算，给大模型提供了可能性", "like": 0}, {"content": "要是手不动就更好了", "like": 0}, {"content": "好了你已经学会transform了 开始做个gpt出来吧[脱单doge]", "like": 2}, {"content": "各位大佬，是不是入门vit得4090起步？", "like": 0}, {"content": "谢谢你，我的ml 入门老师", "like": 0}, {"content": "牛逼 真神", "like": 0}, {"content": "运气贼好，最近刚开始学transformer就看到这个更新，瞬间悟了", "like": 0}, {"content": "讲的是真的牛逼啊", "like": 0}, {"content": "可以讲讲Mamba吗？[热词系列_谢谢老师]", "like": 1}, {"content": "先赞后看，养成习惯", "like": 0}, {"content": "火前留名，必火[打call]", "like": 0}, {"content": "56:20 不成立吧", "like": 0}, {"content": "复习一下[doge]", "like": 0}, {"content": "讲的真不错，第一个从数学，矩阵运算角度给我大概定性的的讲明白了Transform和attention。", "like": 0}, {"content": "up确实很厉害，让我只有线代入门水平的人能看懂一半的内容。并且把线代知识跟几何知识对应起来实在是太棒了，如果大学的时候有这个视频我的线代也不至于学得这么痛苦[大哭]", "like": 0}, {"content": "本科数学专业的，毕业十年之后终于听懂了。", "like": 0}, {"content": "蚌埠，照你这么说啥都是CNN，我还可以说transformer本质是RNN[嗑瓜子][歪嘴]", "like": 1}, {"content": "谢谢老师梳理知识点，三联奉上[给心心]", "like": 0}, {"content": "啥也不说了，attention就完事了[doge]", "like": 0}, {"content": "[doge]", "like": 22}, {"content": "Q和K本身并没有多少数学含义，只是在他们本身所包含的参数和存储的信息才重要的。类似RAG，一个矩阵是模型自己的信息矩阵，一个是输入的查询Q变化的矩阵", "like": 0}, {"content": "牛！！", "like": 0}, {"content": "先点赞👍", "like": 0}, {"content": "知识量太密集了，顶的上我一个月的学习量。听得懂。谢谢王老师。", "like": 0}, {"content": "谢谢老师，把一些我的疑惑点给解决了", "like": 0}, {"content": "是不是应该讲讲和transformer类似的图神经网络呀😄", "like": 3}, {"content": "先tm三连了再说，回家仔细看[打call][打call][打call][打call][打call][打call][打call][打call][打call][打call][打call]", "like": 0}, {"content": "讲得太好了，必须一键三连[给心心][给心心][给心心]", "like": 0}, {"content": "1小时45分12秒的视频，造成我关注力形成了至少9次外向型离散态分布，以及分离矢量化跃迁，最终沉淀下来不符合章节凝聚的外延生成物，导致对整体视频的分解和吸收降低了27.65%的利用率，在没有有效裂解外延生成物的办法下，唯一可用的探索方式，就只有二次离散吸收了。如果上面我没说清楚，我再简单点讲，就是视频时间过长，思想抛锚了多次注意力失去焦点了，一次没法听明白所有的内容，只好听二遍补充一下！[笑哭][笑哭]", "like": 7}, {"content": "能不能出一版正常语速的", "like": 0}, {"content": "你终于更新了", "like": 0}, {"content": "Diffusion[给心心]", "like": 1}, {"content": "终于更新啦[星星眼]", "like": 0}, {"content": "终于等到你！！", "like": 0}, {"content": "出mamba吧，mamba我实在看不懂。", "like": 0}, {"content": "22:21处，up说的是内积保持不变，是不是指内积的关系保持不变，并不是值保持不变？", "like": 1}, {"content": "可以讲讲segformer吗", "like": 0}, {"content": "哇噢噢噢噢噢噢噢，来啦！", "like": 0}, {"content": "深入浅出", "like": 0}, {"content": "太牛逼了 哥", "like": 0}, {"content": "线性代数看B站up主@晓之车高山老师 ，讲的挺不错的", "like": 9}, {"content": "太赞啦！[支持]", "like": 0}, {"content": "还愿来了[脱单doge]", "like": 0}, {"content": "好复杂啊。[难过]", "like": 0}, {"content": "真应该给我导师发过去，次次都要给他重新讲[灵魂出窍]", "like": 0}, {"content": "干货，讲得很好，层层递进，结合例子，捉住本质，让人理解", "like": 0}, {"content": "相见恨晚啊，太流弊了", "like": 0}, {"content": "终于更新了，加油", "like": 0}, {"content": "clip 多模态会讲嘛，求", "like": 0}, {"content": "而你我的朋友 你是真正的疯狗", "like": 0}, {"content": "讲得太好了，很有帮助，加油，一定要做下去", "like": 0}, {"content": "还没看，先投币了！", "like": 0}, {"content": "其实为什么要有Q和V，我是从LSTM那会注意力机制就开始关注注意力机制了，如果只计算一个A当作注意力的值矩阵，那这个网络本质上和传统带注意力机制的LSTM网络是一样的，注意力值是通过输入一次算出来的，这个 A的来源是说不清的，这也就是很多人说传统注意力机制的注意力其实不是注意力的原因。引入Q和K以后，这个注意力的值是和输入密切相关的，这个A是必须经过输入变换得到的，这样得到的注意力更加能够表达各个分量重要性的含义，也能更有说服力。", "like": 2}, {"content": "过于硬核，看不懂[笑哭]", "like": 0}, {"content": "还没功夫学习视频，先投两个币[打call]", "like": 0}, {"content": "无敌无敌", "like": 0}, {"content": "讲的太透了，UR MY HERO", "like": 0}, {"content": "新鲜热乎的，我为什么这么幸运", "like": 0}, {"content": "卧槽，两小时[doge]", "like": 0}, {"content": "nb", "like": 0}, {"content": "为了看懂矩阵部分，我又重学了一遍线性代数，感谢UP带我巩固知识[妙啊]", "like": 0}, {"content": "大赞！！！up的思考给我很大的启发。思路细致、知识设计连贯、逻辑完备、视频质量超高，非常感谢！！！", "like": 1}, {"content": "多更新呀！！！！！！", "like": 0}, {"content": "听注意力机制听着听着注意力就涣散了[笑哭]", "like": 0}, {"content": "豁然开朗，会心一笑。好久没有这种学习一点通的感觉了。", "like": 0}, {"content": "终于等到up主更新了！", "like": 0}, {"content": "大哥，视频太长不利于完播率啊[笑哭]，还这个硬核", "like": 0}, {"content": "听过最好的解释，一直都不理解为什么，就总是导致我没兴趣[笑哭]", "like": 0}, {"content": "太牛了", "like": 1}, {"content": "问一下评论区的大神们，在工程实践上，比如面部识别好了，一般神经网络的隐藏层大概会给多少层，节点给多少个？如果更加复杂的任务，是不是就是力大砖飞，玩命加隐藏层和节点就完事了？", "like": 0}, {"content": "终于等到你啦", "like": 0}, {"content": "大的来了", "like": 0}, {"content": "新年第一更，加油！[奥比岛_点赞]", "like": 0}, {"content": "哇哇！", "like": 0}, {"content": "果然憋了个大的，有的没听懂但还是没听够，哥你说不难的那几个部分可以再细说我真菜，反倒是RNN和核函数那块哥你细看就知道也不是太复杂了", "like": 1}, {"content": "最喜欢的up终于讲的导我的研究方向了[星星眼][星星眼][星星眼][星星眼]", "like": 0}, {"content": "很像我今天发的邮件呐[打call]", "like": 0}, {"content": "时长感人", "like": 0}, {"content": "哇，攒了个大活", "like": 0}, {"content": "木头还是这么干，一个小时四十五分钟，信息密度很高", "like": 0}, {"content": "[呲牙]好久不见啊", "like": 0}, {"content": "transformer类的模型为什么很少存在过拟合呢？性能为什么没有边界？", "like": 10}, {"content": "高质量啊，三连支持！[打call]", "like": 0}, {"content": "这时长..先收藏了", "like": 9}, {"content": "终于等到你！", "like": 0}, {"content": "三连送上[打call][打call]", "like": 0}, {"content": "最近看了好多transformers", "like": 0}, {"content": "一人血书求讲mamba", "like": 10}, {"content": "bilibiliAI视频总结：\nTransformer模型以及注意力机制的本质。作者从Transformer的编码和解码结构入手，探讨了其与CNN和RNN的相似之处。他指出，注意力机制相当于显卡，而编码和解码结构则类似于计算机硬件的冯诺伊曼架构。最后，作者提到了Transformer模型的发展历程和三个分支，并解释了编码和解码结构在机器翻译中的作用。视频提供了对Transformer模型的深入理解和认识。", "like": 1}, {"content": "wc，最近正苦于寻找transformer的资料，还想着为什么这么少资料。结果今天你就发了，好耶！[打call]", "like": 1}, {"content": "终于更新了，泪目[大哭]", "like": 0}, {"content": "太猛了1个小时45分钟", "like": 0}, {"content": "想系统学习下ai及大模型，有推荐的课程或视频吗，是计算机专业但无ai基础", "like": 1}, {"content": "谢谢，希望能看懂", "like": 0}, {"content": "硬核干货[星星眼][星星眼]", "like": 0}, {"content": "终于来了！哥！\n(另外diffusion什么时候出[害羞])", "like": 51}, {"content": "来啦！", "like": 1}, {"content": "啥时候出mamba[doge]", "like": 27}, {"content": "火星了，现在是mamba的时代[doge]", "like": 0}, {"content": "等了好久 先投币了", "like": 0}, {"content": "这时长[doge]", "like": 0}, {"content": "木头爹!!!!!", "like": 0}, {"content": "太及时了哥们，三连了", "like": 0}, {"content": "哇木头老师，终于等来了[打call]", "like": 0}, {"content": "点赞投币[打call]", "like": 0}, {"content": "666，哥，你讲注意力机制了[星星眼]", "like": 4}, {"content": "一、transformer和注意力机制的本质，以及编码和解码的结构及其与注意力机制的关系，帮助理解transformer的优势和应用场景。\n00:01 - 介绍transformer是人工智能主流技术，大语言模型GBT是在其基础上做出来的。\n01:20 - 从transformer的大结构开始理解，注意力机制和编解码结构是理解的关键。\n03:04 - 编解码结构和注意力机制的关系类似于计算机硬件里的冯诺依曼架构和显卡之间的关系。\n二、分词器和编码器在处理语义时的缺陷，提出了使用前空间来协调编解码的想法，并解释了矩阵和空间变换的关系。\n08:21 - 编解码的码需要数字化和数字化后的数值体现语义相对关系\n09:18 - 独热编码的问题是信息密度过于稀疏，无法体现token之间的语义联系\n11:09 - 矩阵相乘可以看作是一种空间变换，对transformer来说非常重要\n三、向量和矩阵相乘的效果，以及二次型的概念和矩阵的行列式的意义。同时也讨论了线性代数中的一些概念和应用。\n16:40 - 矩阵的乘法是线性变化过程，代表空间变换\n17:54 - 向量和矩阵相乘可以对应到新空间里的图像，矩阵表示空间变换\n19:20 - 线性代数应该先介绍矩阵和空间变换的对应关系，避免行列式的积角旮旯概念\n四、神经网络和矩阵的区别，以及如何通过增加神经元实现升维和降维，以及编码和解码的过程。\n25:00 - 神经网络和矩阵的区别\n26:43 - 隐藏层的作用是让模型更复杂，数据升维操作\n28:32 - 神经网络的层数代表了对数据特征进行抽象的程度\n五、前空间的概念和如何通过机器学习的方法将真实语言中的token投射到前空间中，以及编码和解码的原理和训练方法。\n33:20 - 卷积神经网络(CNN)的升维和降维过程\n33:58 - 前空间和翻译手册的区别，可连续和对应关系\n36:13 - 谷歌论文提出的两种调整思路：COO和skip gram\n六、机器翻译中注意力机制的原理和计算方式，通过对词向量进行升维和降维操作，计算出注意力得分并进行缩放，最终得到输出词向量。\n41:41 - 编解码部分\n--本内容由AI视频小助理生成，关注解锁AI助理，由@莫白四副 召唤发送", "like": 12}, {"content": "就等你了[星星眼][星星眼][星星眼]", "like": 0}, {"content": "爷爷，你关注的up更新了", "like": 0}, {"content": "超长 辛苦了", "like": 1}, {"content": "好哇，以后还真可能用得上[OK]", "like": 1}, {"content": "爽啊！", "like": 1}, {"content": "你这太难产了，先投币支持一波[笑哭]", "like": 0}, {"content": "你真打算教会我？[吃瓜]", "like": 0}, {"content": "都快忘了有这个up主了[doge]", "like": 0}, {"content": "厉害", "like": 0}, {"content": "来了来了！", "like": 0}, {"content": "[星星眼][星星眼][星星眼][星星眼]最喜欢的一集", "like": 0}, {"content": "还是阿b的推荐算法懂我啊，立马就推送了[蔚蓝档案表情包_想要]", "like": 1}, {"content": "救人于水火，泪目", "like": 0}, {"content": "直接先投币", "like": 0}, {"content": "前排出售爆米花和哈根达斯[墨镜]", "like": 0}, {"content": "等我有空再看，一键三连了[脱单doge]", "like": 1}, {"content": "木头哥，牛逼！", "like": 0}, {"content": "终于想起了b站密码？", "like": 0}, {"content": "终于等到transformer和Attention了，类目", "like": 10}, {"content": "猜一下视频长度[doge]", "like": 0}, {"content": "太牛了，必须投币！", "like": 0}, {"content": "这么久[星星眼]", "like": 0}, {"content": "更新了，支持支持，好看，爱看[打call]", "like": 0}, {"content": "这视频长度[脸红]", "like": 0}, {"content": "冲冲冲[doge]", "like": 0}, {"content": "终于更新了支持", "like": 0}], "cid": 1497135272, "hashtag": [{"tag_id": 46183, "tag_name": "人工智能", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 1697, "tag_name": "教程", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 2903800, "tag_name": "Transformer", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 3427122, "tag_name": "Attention", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 19254871, "tag_name": "位置编码", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 409938, "tag_name": "注意力", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 392328, "tag_name": "机器学习", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 1291647, "tag_name": "深度学习", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 3110916, "tag_name": "卷积", "music_id": "", "tag_type": "old_channel", "jump_url": ""}, {"tag_id": 3561660, "tag_name": "word2vec", "music_id": "", "tag_type": "old_channel", "jump_url": ""}], "subtitles": {"allow_submit": false, "lan": "", "lan_doc": "", "subtitles": []}}